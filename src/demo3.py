import os
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_community.embeddings import FastEmbedEmbeddings
from langchain_core.prompts import ChatPromptTemplate, format_document
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_openai import ChatOpenAI

#from src.demo1 import prompt_template
#deepseekçš„æ¨¡å‹
api_key = os.environ.get("OPENAI_API_KEY")
if not api_key:
 raise RuntimeError("æœªæ£€æµ‹åˆ° API Keyã€‚è¯·å…ˆåœ¨å½“å‰çª—å£æ‰§è¡Œ: set DEEPSEEK_API_KEY=ä½ çš„å¯†é’¥ æˆ– set OPENAI_API_KEY=ä½ çš„å¯†é’¥")
client = ChatOpenAI(
    model = "deepseek-chat",
    api_key=os.environ.get('sk-e9db8e390cbf4a2bbe48ed154479a427'),
    base_url="https://api.deepseek.com")

# å¯é€‰ï¼šå¦‚æœä½ ä»ç„¶éœ€è¦ä»£ç†ç”¨äºå…¶ä»–ç½‘ç»œè¯·æ±‚ï¼Œå¯ä»¥ä¿ç•™ï¼›ä¸ FastEmbed æ— å…³
os.environ["http_proxy"] = "http://127.0.0.1:7890"
os.environ["https_proxy"] = "http://127.0.0.1:7890"

# ---------------------- 1. å‡†å¤‡ç¤ºä¾‹æ–‡æ¡£ ----------------------
documents = [
    Document(page_content="ç‹—æ˜¯ä¼Ÿå¤§çš„ä¼´ä¾£ï¼Œä»¥å…¶å¿ è¯šå’Œå‹å¥½è€Œé—»åã€‚", metadata={"source": "å“ºä¹³åŠ¨ç‰©å® ç‰©æ–‡æ¡£"}),
    Document(page_content="çŒ«æ˜¯ç‹¬ç«‹çš„åŠ¨ç‰©ï¼Œé€šå¸¸å–œæ¬¢è‡ªå·±çš„ç©ºé—´ã€‚", metadata={"source": "å“ºä¹³åŠ¨ç‰©å® ç‰©æ–‡æ¡£"}),
    Document(page_content="é‡‘é±¼æ˜¯åˆå­¦è€…çš„æµè¡Œå® ç‰©ï¼Œéœ€è¦ç›¸å¯¹ç®€å•çš„æŠ¤ç†ã€‚", metadata={"source": "é±¼ç±»å® ç‰©æ–‡æ¡£"}),
    Document(page_content="é¹¦é¹‰æ˜¯èªæ˜çš„é¸Ÿç±»ï¼Œèƒ½å¤Ÿæ¨¡ä»¿äººç±»çš„è¯­è¨€ã€‚", metadata={"source": "é¸Ÿç±»å® ç‰©æ–‡æ¡£"}),
    Document(page_content="å…”å­æ˜¯ç¤¾äº¤åŠ¨ç‰©ï¼Œéœ€è¦è¶³å¤Ÿçš„ç©ºé—´è·³è·ƒã€‚", metadata={"source": "å“ºä¹³åŠ¨ç‰©å® ç‰©æ–‡æ¡£"}),
]

# ---------------------- 2. ä½¿ç”¨ FastEmbed å‘é‡æ¨¡å‹ ----------------------
# æ³¨ï¼šFastEmbedEmbeddings é»˜è®¤æ¨¡å‹ä¸º BAAI/bge-small-zh-v1.5ï¼ˆä¸­æ–‡è¡¨ç°è‰¯å¥½ï¼‰
# ä¹Ÿå¯è‡ªå®šä¹‰ï¼šFastEmbedEmbeddings(model_name="BAAI/bge-base-zh-v1.5")
# embedding_model = FastEmbedEmbeddings()  # ä¸­æ–‡æ¨èï¼Œå…ä¸‹è½½/å…ä»£ç†ï¼ŒCPU å¿«
print("åˆå§‹åŒ– FastEmbedEmbeddingsï¼ˆå¦‚é¦–æ¬¡è¿è¡Œï¼Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰...")
embedding_model = FastEmbedEmbeddings(model_name="qdrant/bge-small-zh-v1.5-onnx-q")
print("FastEmbed åˆå§‹åŒ–å®Œæˆã€‚")
# ---------------------- 3. åˆ›å»º Chroma å‘é‡æ•°æ®åº“ ----------------------
# æç¤ºï¼šé¦–æ¬¡è¿è¡Œä¼šåœ¨æœ¬åœ°å†…å­˜ä¸­æ„å»ºï¼›è‹¥éœ€æŒä¹…åŒ–ï¼Œå¯åŠ  persist_directory="./chroma_db"
vector_store = Chroma.from_documents(documents, embedding=embedding_model)

# ---------------------- 4. æ‰§è¡Œç›¸ä¼¼åº¦æ£€ç´¢ ----------------------
query = "å’–å•¡çŒ«"
results = vector_store.similarity_search_with_score(query, k=3)

# ---------------------- 5. è¾“å‡ºç»“æœ ----------------------
print("ğŸ” æŸ¥è¯¢é—®é¢˜ï¼š", query)
print("\nğŸ“š ç›¸ä¼¼å†…å®¹åŒ¹é…ç»“æœï¼š\n")
for doc, score in results:
    print(f"å†…å®¹ï¼š{doc.page_content}")
    print(f"æ¥æºï¼š{doc.metadata.get('source', 'æœªçŸ¥')}")
    print(f"ç›¸ä¼¼åº¦åˆ†æ•°ï¼š{score}")
    print("-" * 50)

#æ£€ç´¢å™¨ bind(k=1) è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„ç¬¬ä¸€ä¸ª
retriever = RunnableLambda(vector_store.similarity_search).bind(k=1)

# print(retriever.batch(['å’–å•¡çŒ«', 'é²¨é±¼']))

#æç¤ºæ¨¡æ¿
message = """
ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ä»…å›ç­”è¿™ä¸ªé—®é¢˜
{question}
ä¸Šä¸‹æ–‡ï¼š
{context}
"""

prompt_temp = ChatPromptTemplate.from_messages([('human',message)])

#RunnablePassthrough
chain = {'question': RunnablePassthrough(), 'context': retriever | RunnableLambda(format_docs),} | prompt_temp | client

resp = chain.invoke('è¯·ä»‹ç»ä¸€ä¸‹çŒ«ï¼Ÿ')

print(resp.content)